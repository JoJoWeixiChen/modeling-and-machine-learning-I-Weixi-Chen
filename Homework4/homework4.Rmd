---
title: "Homework4"
author: "Weixi Chen"
date: "2/16/2022"
output: github_document
---

## Exercise 4

### a

Considering that X is evenly distributed, so the fraction of available observations we will use is 10%.

### b

We use 1% of the available dataset on average, as we use 10% data from both X1 and X2, and then we will use 10% * 10% = 1% data on average.

### c

We will use $(10\%)^{100}$ of data on average, and the reason is the same as the question b.

### d

Every time when the dimension, p, increase by 1, the available data we use for predicting observation will decrease by 10%. So, when the dimension p is very large, there will be few training observations near any given test observations.

### e

To make sure that we can use 10% of the available training data, we should let 10% of data be included into the hypercube. So the length of each side should meet the condition:

$l^{100} = 10%$

And we can get the result as: $l = \sqrt[100]{10\%}$


## Exercise 10

### a

```{r}
library(ISLR2)
data(Weekly)
```

```{r}
summary(Weekly[, -9])
```

```{r}
pairs(Weekly[, -9])
```

```{r}
cor(Weekly[, -9])
```

We can find that there strong correlation (0.842) between variable 'Year' and 'Volume', and the correlations between other variables are weak.

### b

```{r}
summary(Weekly$Direction)
```

```{r}
logistic1 <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = Weekly, family = 'binomial')
summary(logistic1)
```

From the summary of result, we can find out that only the variable 'Lag2' is statistically significant at the 5% confidence level.

### c

```{r}
logistic1_pred <- predict(logistic1, type = "response")
logistic1_result <- lapply(logistic1_pred, function(x) ifelse(x > 0.5, 'Up', 'Down'))
table(as.vector(unlist(logistic1_result)), Weekly$Direction)
```

From the table above, we can find out that the mistake that the logistic model made is that it always recognized the 'Down' direction as 'Up'. And the accuracy is (54 + 557)/1089 = 56.1%

### d

```{r}
train_data = Weekly[Weekly[, 'Year'] < 2009, ]
test_data = Weekly[Weekly[, 'Year'] > 2008, ]
```

```{r}
logistic2 <- glm(Direction ~ Lag2, data = train_data, family = 'binomial')
logistic2_pred <- predict(logistic2, test_data[, -9], type = "response")
logistic2_result <- lapply(logistic2_pred, function(x) ifelse(x > 0.5, 'Up', 'Down'))
table(as.vector(unlist(logistic2_result)), test_data$Direction)
```

We can see that the fraction of correct predictions is (9 + 56)/104 = 62.5%

### e

```{r}
library(MASS)
lda1 <- lda(Direction ~ Lag2, train_data)
lda1_pred <- predict(lda1, test_data)
table(lda1_pred$class, test_data$Direction)
```

The overall fraction of correct predictions is (9 + 56)/104 = 62.5%

### f

```{r}
qda1 <- qda(Direction ~ Lag2, train_data)
qda1_pred <- predict(qda1, test_data)
table(qda1_pred$class, test_data$Direction)
```

The overall fraction of correct predictions is (0 + 61)/104 = 58.7%

### g

```{r}
library(class)
set.seed(1)
train_data1 = as.matrix(train_data[, 'Lag2'])
train_direction = train_data$Direction
test_data1  = as.matrix(test_data[, 'Lag2'])
knn1_pred <- knn(train_data1, test_data1, train_direction, k = 1)
table(knn1_pred, test_data$Direction)
```

The overall fraction of correct predictions is (21 + 31)/104 = 50%

### h

```{r}
library(e1071)
naiveBayes1 <- naiveBayes(train_data1, train_direction, laplace = 0)
naiveBayes_pred <- predict(naiveBayes1, test_data1, type = "class")
table(naiveBayes_pred, test_data$Direction)
```

The overall fraction of correct predictions is (61 + 0)/104 = 58.7%

### i

The logistics model and LDA methods seem to provide the best prediction results on this data.

### j

Considering use variable "Lag1", "Lag2", and the interaction of "Lag1" and "Lag2".

#### Logistic model

```{r}
logistic3 <- glm(Direction ~ Lag1 + Lag2 + Lag1:Lag2, data = train_data, family = 'binomial')
logistic3_pred <- predict(logistic3, test_data[, -9], type = "response")
logistic3_result <- lapply(logistic3_pred, function(x) ifelse(x > 0.5, 'Up', 'Down'))
table(as.vector(unlist(logistic3_result)), test_data$Direction)
```

The overall fraction of correct predictions is (53 + 7)/104 = 57.7%

#### LDA model

```{r}
library(MASS)
lda2 <- lda(Direction ~ Lag1 + Lag2 + Lag1:Lag2, train_data)
lda2_pred <- predict(lda2, test_data)
table(lda2_pred$class, test_data$Direction)
```

The overall fraction of correct predictions is (7 + 53)/104 = 57.7%

#### QDA model

```{r}
qda2 <- qda(Direction ~ Lag1 + Lag2 + Lag1:Lag2, train_data)
qda2_pred <- predict(qda2, test_data)
table(qda2_pred$class, test_data$Direction)
```

The overall fraction of correct predictions is (23 + 25)/104 = 46.2%

#### knn k = 10

```{r}
library(class)
set.seed(2)
knn2_pred <- knn(train_data1, test_data1, train_direction, k = 10)
table(knn2_pred, test_data$Direction)
```

The overall fraction of correct predictions is (18 + 40)/104 = 55.8%

The logistic model and LDA model still seems to be the best models.